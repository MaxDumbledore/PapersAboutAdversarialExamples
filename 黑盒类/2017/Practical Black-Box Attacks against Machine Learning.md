# 针对机器学习的实用黑盒攻击

## 摘要

机器学习（ML）模型，例如深度神经网络（DNN），容易受到敌对例子的攻击：修改恶意输入以产生错误的模型输出，而对人类观察者来说则是未经修改的。潜在的攻击包括将恶意软件等恶意内容识别为合法或控制车辆行为。然而，所有现有的对抗性示例攻击都需要了解模型内部或其训练数据。我们介绍了攻击者在不了解此类信息的情况下控制远程托管DNN的第一个实际演示。事实上，我们的黑匣子对手唯一的能力就是观察DNN给选定输入的标签。我们的攻击策略包括训练一个局部模型来替代目标DNN，使用由对手合成并由目标DNN标记的输入。我们使用本地替代品来制作对抗性示例，并发现它们被目标DNN误判。为了进行真实世界的正确盲评估，我们攻击了在线深度学习API MetaMind托管的DNN。我们发现，他们的DNN错误分类了84.24%用我们的替代品制作的对抗性示例。我们通过使用逻辑回归替代品对Amazon和Google托管的模型进行相同的攻击，展示了我们的策略对许多ML技术的普遍适用性。亚马逊和谷歌分别以96.19%和88.94%的比率错误分类了两个具有对抗性的例子。我们还发现，这种黑匣子攻击策略能够规避之前发现的防御策略，从而使对抗性示例制作变得更加困难。