# 神经网络鲁棒性的评价

## 摘要

神经网络为大多数机器学习任务提供了最先进的结果。不幸的是，神经网络容易受到敌对示例的攻击：给定输入x和任何目标分类t，可能会找到一个与x相似但分类为t的新输入x 0。这使得神经网络难以应用于安全关键领域。防御蒸馏是最近提出的一种方法，它可以采用任意神经网络，并增强其鲁棒性，将当前攻击的成功率从95%降低到0.5%

在本文中，我们通过引入三种新的攻击算法，证明了防御蒸馏并不能显著提高神经网络的鲁棒性，这三种算法在蒸馏和未蒸馏的神经网络上都是成功的，概率为100%。我们的攻击是根据之前文献中使用的三个距离度量定制的，与以前的对抗性示例生成算法相比，我们的攻击通常更有效（而且从来没有更糟）。此外，我们建议在一个简单的可转移性测试中使用高置信度的对抗性示例，我们证明它也可以用来打破防御蒸馏。我们希望我们的攻击将被用作未来防御尝试的基准，以创建能够抵抗对抗性示例的神经网络

## 5. 我们的方法

现在我们转向构建对抗性示例的方法。首先，我们依赖于对抗性示例的初始公式[46]，并正式定义了为图像x查找对抗性实例的问题，如下所示：

![image-20220331185604386](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331185604386.png)

其中x是固定的，目标是找到使D（x，x+δ）最小的δ。也就是说，我们想找到一些小的变化δ，我们可以对图像x进行一些小的改变，改变其分类，但结果仍然是有效的图像。这里D是一些距离度量；对我们来说，它将是L0、L2或L∞ 如前所述

我们通过将其表述为一个合适的优化实例来解决这个问题，现有的优化算法可以解决这个问题。有很多方法可以做到这一点；我们探索方法的空间，并根据经验确定哪些方法会导致最有效的攻击。

### A  目标函数

由于约束C（x+δ）=t是高度非线性的，现有算法很难直接求解上述公式。

因此，我们用一种更适合优化的不同形式来表达它。我们定义了一个目标函数f，使得C（x+δ）=t**当且仅当**f（x+δ）≤ 0。f有很多可能的选择：

![image-20220331191328143](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331191328143.png)

其中s是正确的分类，（e）+是max（e，0）的缩写，softplus（x）=log（1+exp（x）），$\text{loss}_{F,s}(x)$是x的交叉熵损失。

请注意，我们已经通过添加一个常数来调整了上面的一些公式；我们这样做只是为了使函数尊重我们的定义。这不会影响最终结果，因为它只是缩放最小化函数

现在，与其把问题描述为

![image-20220331192000172](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331192000172-16487256010371.png)

我们使用另一种描述

![image-20220331192028765](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331192028765-16487256301702.png)

---

![image-20220331192619774](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331192619774.png)

图2. 对常数c的敏感性。我们绘制了通过梯度下降计算的对抗性示例的L2距离，作为目标函数f6的函数c。当c小于0.1时，攻击很少成功。**c>1后，攻击的效果会降低，但总是成功的**。

---

其中c>0是一个适当选择的常数。这两者是等价的，因为存在c>0，因此后者的最优解与前者的最优解相匹配。在用lp范数实例化距离度量D之后，问题变成：给定x，找到δ满足

![image-20220331192851894](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331192851894.png)

根据经验，我们发现选择c的最佳方法**通常是**使用c的最小值，对应的解$x^*$满足$f(x^*)\le 0$。这会导致梯度下降同时最小化两个项，而不是只选择一个项进行优化。

我们通过运行f6公式（我们发现最有效）来验证这一点，该公式适用于MNIST数据集上c=0.01到c=100之间均匀分布的c值（在对数尺度上）。我们在图2中绘制了这条线。

此外，我们还发现，如果选择最小的c，那么f（x∗ ) ≤ 0时，解决方案和最佳方案相差5%，在70%情况下；相差30%，在98%情况下，其中“最佳”指的是使用c的最佳值找到的解决方案。因此，在我们的实现中，我们使用修改的二分搜索来选择c。

### B 盒约束

为了确保修改产生有效的图像，我们对δ有一个限制：我们必须对于所有的$i$有$0\le x_i+\delta_i\le 1$。在优化文献中，这被称为“盒约束”。之前的工作使用了一种特殊的优化算法L-BFGS-B，它原生支持盒约束。

我们研究了三种不同的方法来解决这个问题

1. 投影梯度下降（PGD）：执行标准梯度下降的一步，然后将所有坐标剪辑到框内

   这种方法对于梯度下降方法来说是很差的，这类方法具有复杂的更新步骤（例如，具有动量的方法）：当我们剪辑实际的$x_i$时，我们意外地将输入改变到算法的下一迭代。

2. 剪辑梯度下降：在每一次迭代时修建$x_i$；相反，它将剪辑合并到目标函数中以最小化。换句话说，我们将f（x+δ）替换为f（min（max（x+δ，0），1））

   在解决投影梯度问题的主要问题时，剪辑引入了一个新的问题：算法可以在陷入了平坦点，在该点上，它增加了$x_i$的某些分量，从而大大超过了允许的最大值。当发生这种情况时，偏导数变为零，因此即使通过减少$x_i$进行某种改进是可能的，梯度下降也无法检测到这一点。

3. 变量代换：引入了一个新的变量w，而不是在上面定义的变量δ上进行优化，我们应用变量的改变并在w上进行优化，设置

   ![image-20220331195025932](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331195025932.png)

   因为$-1\le \tanh(w_i)\le 1$，于是有$0\le x_i+\delta_i\le 1$，所以解是合法的。

   我们可以将这种方法视为剪裁梯度下降的平滑，消除了在极端区域卡住的问题。

这些方法允许我们使用其他优化算法，这些算法本身不支持框约束。我们几乎只使用Adam[23]优化器，因为我们发现它在快速找到对抗性示例方面最有效。我们尝试了三种解算器——标准梯度下降法、动量梯度下降法和Adam——这三种解算器都产生了相同质量的解。然而，亚当比其他人收敛得更快；

### C 评估方法

对于每一个可能的目标函数f（·）和实施框约束的方法，我们评估了发现的对抗性示例的质量。

---

![image-20220331200040204](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331200040204.png)

表3. 评估七个可能目标函数之一与三个框约束编码之一的所有组合。我们展示了**平均L2失真**、标准偏差和**成功概率**（可以找到对抗性示例的实例的分数）。对**1000个随机实例**进行评估。当成功率不是100%时，平均值仅适用于成功的攻击。

---

为了选择最优的c，我们在c上执行20次二进制搜索迭代。对于每个选定的c值，我们使用Adam优化器运行**10000次梯度下降迭代**。

该分析的结果见表三。我们评估了MNIST和CIFAR数据集上发现的对抗性示例的质量。两个数据集之间每个目标函数的相对顺序是相同的，因此为了简洁起见，我们只报告MNIST的结果。

最好的目标函数和最差的目标函数在质量上有三个不同的因素。对于最佳最小化函数，处理框约束的方法的选择不会对结果的质量产生显著影响。事实上，表现最差的目标函数，交叉熵损失，是之前文献[46]，[42]中建议最多的方法

为什么有些损失函数比其他的好？当c=0时，梯度下降将不会使任何移动远离初始图像。然而，较大的c通常会导致梯度下降的初始步骤以过于贪婪的方式执行，**只沿着最容易减少f的方向移动**，而忽略D损失，从而导致梯度下降找到次优解。

这意味着，对于损失函数f1和f4，在梯度下降搜索的整个过程中，没有有用的良好常数c。由于常数c加权了距离项和损失项的相对重要性，为了使固定常数c有用，**这两项的相对值应保持大致相等**。这两个损失函数的情况并非如此。

为了解释为什么会出现这种情况，我们必须进行旁白讨论，以分析对抗性例子是如何存在的。考虑网络上的有效输入x和对抗实例$x'$。

当我们从x到x'进行线性插值时，它是什么样子的？也就是说，让$y=\alpha x+(1-\alpha )x'\ \alpha\in [0,1]$。事实证明，Z（·）t的值从输入到对抗性示例基本上是线性的，因此F（·）t大致上是logistic函数

我们通过使用我们的方法在MNIST和CIFAR数据集上的前1000张测试图像上构建对抗性示例来验证这一事实，并发现皮尔逊相关系数r>0.9。

考虑到这一点，考虑损失函数F4（F1的参数是相似的）。为了让梯度下降攻击最初做出任何改变，常数c必须足够大，以

![image-20220331205207711](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331205207711.png)

或者说，当$\epsilon$→ 0,

![image-20220331205405454](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331205405454.png)

这意味着c必须大于梯度的倒数才能取得进展，但f1的梯度与F（·t）相同，因此在初始图像周围很小，这意味着c必须非常大

然而，一旦我们离开初始图像的紧邻区域，图像的梯度∇f1（x+δ）以指数速率增加，使得大常数c导致梯度下降以过度贪婪的方式执行。

我们通过经验验证了所有这些理论。当我们运行攻击时，尝试从10个常数中选择常数−10至$10^{10}$，损失函数f4的平均常数为$10^6$。

有效图像周围损失函数f1的平均梯度为$2^{-20}$但是，在最接近的对抗性例子中为$2^{-1}$。这意味着c比它必须的大一百万倍，导致损失函数f4和f1的性能比其他任何函数都差

### D 离散化

我们将像素强度建模为[0,1]范围内的（连续）实数。然而，在有效图像中，每个像素强度必须是{0，1，…，255}范围内的（离散）整数

我们的公式中没有包含这一额外要求。在实践中，我们忽略完整性约束，解决连续优化问题，然后四舍五入到最接近的整数：第i个像素的强度变为$\lfloor255(x_i+\delta_i)\rceil$

这种舍入将略微降低对抗性示例的质量。如果需要恢复攻击质量，我们可以通过每次改变一个像素值，在离散解定义的晶格上执行贪婪搜索。对于我们的任何攻击，这种贪婪的搜索从未失败。

之前的工作基本上忽略了完整性约束。例如，当使用$\epsilon$=0.1的快速梯度符号攻击时（即，将像素值更改10%），离散化很少影响攻击的成功率。相比之下，在我们的工作中，我们能够找到对图像进行小得多的更改的攻击，**因此离散化效果不能忽略**。我们总是注意生成有效的图像；在报告我们的攻击成功率时，它们总是针对包括离散化后处理的攻击。

## 6. 我们的三种攻击

### A 我们的L2攻击

把这些想法放在一起，我们得到了一种在L2度量中寻找具有低失真的对抗性示例的方法。给定x，我们选择一个目标类t（$t\neq C^*(x)$ ）然后搜索解决问题的$w$

![image-20220331212623526](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331212623526.png)

其中$f$被定义为

![image-20220331212707241](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331212707241.png)

该f基于之前发现的最佳目标函数，稍加修改，以便我们可以通过调整κ来控制错误分类发生的置信度。参数κ鼓励解算器找到一个敌对实例x 0，该实例将被高度置信地归类为t类。

我们为我们的攻击设置κ=0，但我们注意到，这个公式的一个副作用是它允许我们控制期望的置信度。这将在第VIII-D节中进一步讨论。

---

![image-20220331221720153](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331221720153.png)

Fig3. 我们的L2对手应用MNIST数据集，对每个源/目标对执行有针对性的攻击。每个数字都是数据集中具有该标签的第一个图像

---

图3显示了针对每个源数字和目标数字应用于我们的MNIST模型的这种攻击。几乎所有的攻击在视觉上都与原始数字无法区分。

CIFAR的一个可比数字（图12）见附录。没有攻击可以从视觉上与基线图像区分开来

多起点梯度下降：梯度下降法的主要问题是，它的贪婪搜索不能保证找到最优解，并且可能陷入局部极小值。为了解决这个问题，我们选择了多个接近原始图像的随机起点，并从这些点中的每一个点进行梯度下降，进行固定次数的迭代。我们从半径为r的球上随机均匀地采样点，其中r是迄今为止发现的最接近对手的例子。从多个起点开始可以降低梯度下降陷入糟糕的局部极小值的可能性

### B 我们的L0攻击

L0距离度量是不可微的（注意$L_0$是向量中非0元素的个数），因此不适合标准梯度下降。相反，我们使用一种迭代算法，在每次迭代中，识别一些对分类器输出没有太大影响的像素，然后固定这些像素，因此它们的值永远不会改变。固定像素集在每次迭代中都会增长，直到通过消除过程，我们确定了一个近尽量小的（但可能不是最小）像素子集，可以对其进行修改以生成一个对抗性示例。在每次迭代中，我们使用L2攻击来确定哪些像素不重要。

更详细地说，在每次迭代中，我们称之为L2敌手，仅限于修改允许集合中的像素。让$\delta$为$L_2$敌手返回的解，$x+\delta$为对应的对抗样本。我们计算$g=\nabla f(x+\delta)$（**目标函数**的梯度，在对抗情况下评估）。我们选择像素$i=\arg\min_i g_i\delta_i$，将i从允许的集合中移除（对f函数影响最小的像素点）。直觉是$g_i\delta_i$告诉我们当从x移动到x+δ时，从图像的第i个像素到f（·）的减少量：gi告诉我们每单位变化到第i个像素，f的减少量是多少，我们乘以第i个像素的变化量。这个过程会重复，直到$L_2$敌手找不到对抗样本。

最后一个细节是：选择好L2敌手选择的常数c。我们一开始将c设置为非常小的值$10^{-4}$。然后我们以这个c值运行我们的L2对手。如果失败，我们加倍c，然后再试一次，直到成功。如果c超过**固定阈值**（例如$10^{10}$），我们将中止搜索

JSMA会增加一组（最初为空）允许更改的像素，并将像素设置为最大化总损失。相比之下，我们的攻击会缩小允许更改的像素集（最初包含每个像素）

我们的算法比JSMA有效得多（评估见第七节）。它也很高效：我们引入了优化，使其速度与我们的L2攻击一样快，在MNIST和CIFAR上只有一个起点；它在ImageNet上的速度要慢得多。我们不是在每次迭代中从初始图像开始梯度下降，而是从上一次迭代中找到的解开始梯度下降（“温启动”）。这大大减少了每次迭代过程中所需的梯度下降轮数，**因为保持k像素不变的解决方案通常与保持k+1像素不变的解决方案非常相似**。

图4显示了针对MNIST数据集上每个目标类的每个源类的一位数的L0攻击。这些攻击在视觉上是显而易见的，这意味着L0攻击比L2更难。也许最糟糕的情况是7被归类为6；有趣的是，这种针对L2的攻击是仅有的视觉上可区分的攻击之一。

---

![image-20220331224044266](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331224044266-16487376447601.png)

图4。我们的L0对手应用MNIST数据集，对每个源/目标对执行有针对性的攻击。每个数字都是具有该标签的数据集中的第一个图像。

---

CIFAR的可比数字（图11）见附录

### C 我们的L$\infty$攻击

L∞ 距离度量不是完全可微的，标准的梯度下降法也不能很好地解决这个问题。我们进行了原始的优化实验

![image-20220331224230386](Towards Evaluating the Robustness of Neural Networks.assets\image-20220331224230386-16487377531542.png)

然而，我们发现梯度下降产生了非常糟糕的结果：$||\delta||$只惩罚δ中最大的（绝对值）条目，对其他条目没有影响。因此，梯度下降很快就会在**两个次优解之间振荡**。考虑一个δi＝0.5和δj＝0.5-$\epsilon$的情形。$L_\infty$只会惩罚$\delta_i$而不是$\delta_j$，$\frac{\partial}{\part \delta_j}||\delta||_\infty$将会是0。因此，梯度不会对增加δj施加惩罚，即使它已经很大了。在下一次迭代中，我们可能会移动到δj略大于δi的位置，$\delta_i=0.5-\epsilon'$和$\delta_j=0.5+\epsilon''$，我们起点的镜像。换句话说，梯度下降可能会在δi=δj=0.5的直线上来回振荡，使得几乎不可能进行前进。

我们使用迭代攻击来解决这个问题。我们将目标函数中的L2项替换为超过$\tau$的任何项的惩罚（最初为1，在每次迭代中减少）。这可以防止振荡，因为这个损失项会同时惩罚所有较大的值。具体来说，在每次迭代中，我们都要解决

![image-20220402144852583](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402144852583.png)

每次迭代后，如果所有i的δi<τ，我们将τ减少0.9倍并重复；否则，我们将终止搜索。

同样，我们必须选择一个好的常数c来表示L∞ 对手。我们采用与L0攻击相同的方法：最初将c设置为非常低的值，然后运行L∞ 这个c值的对手。如果失败，我们加倍c，然后再试一次，直到成功。如果c超过固定阈值，我们将中止搜索

在每次迭代中使用梯度下降的“热启动”，该算法的速度大约与我们的L2算法一样快（只有一个起点）。

图5显示了L∞ 攻击应用于MNSIT数据集上每个源类的一位数，目标是每个目标类。虽然大多数差异在视觉上并不明显，但也有一些差异是明显的。同样，最糟糕的情况是7被归类为6

---

![image-20220402145307732](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402145307732.png)

图5。我们的L∞ 对手应用于MNIST数据集，对每个源/目标对执行有针对性的攻击。每个数字都是数据集中具有该标签的第一个图像

---

CIFAR的一个可比数字（图13）见附录。没有攻击可以从视觉上与基线图像区分开来

## 7. 攻击评估

我们将我们的目标攻击与之前的出版物中预先报道的三个距离指标中的每一个的最佳结果进行比较

我们重新实现了Deepfool、快速梯度符号和迭代梯度符号。对于快速梯度符号，我们在$\epsilon$上搜索，以找到产生对抗性示例的最小距离；如果没有生成目标类，则返回失败。我们的迭代梯度符号方法类似：我们在$\epsilon$（固定α=1/256）上搜索并返回最小值

对于JSMA，我们使用CleverHans[35]中的实现，只需稍加修改（我们将性能提高了50倍，而不影响准确性）。

由于固有的巨大计算成本，JSMA无法在ImageNet上运行：回想一下，JSMA搜索一对像素p，q，它们可以一起改变，从而使目标类更可能出现，而其他类不太可能出现。ImageNet将图像表示为299×299×3个向量，因此在所有像素对上进行搜索需要在计算的每个步骤上进行$2^{36}$项工作。如果我们删除对像素的搜索，JSMA的成功率会显著下降。因此，我们在ImageNet上报告它总是失败

如果攻击产生了一个具有正确目标标签的对抗性示例，无论需要进行多少更改，我们都会报告成功。失败表示攻击完全无法成功。

我们在CIFAR和MNSIT上对测试集中的前1000张图像进行评估。在ImageNet上，我们报告了Inception v3版最初正确分类的1000张图像。在ImageNet上，我们通过随机选择100个目标类别（10%）来近似最佳情况和最坏情况的结果。

MNIST和CIFAR的结果见表IV，ImageNet的结果见表V

对于所有三个数据集的每一个距离度量，我们的攻击都能找到比之前最先进的攻击更接近的敌对示例，我们的攻击从来都能找到敌对示例。我们的L0和L2攻击找到了比最好的预先发布的攻击失真低2到10倍的敌对示例，并以100%的概率成功。我们的L∞ 攻击的质量与之前的工作相当，但成功率更高。我们的L∞ 对ImageNet的攻击非常成功，我们只需翻转每个像素的最低位，就可以将图像的分类更改为任何所需的标签，这是一种视觉上无法检测到的更改。

随着学习任务变得越来越困难，由于模型的复杂性，以前的攻击会产生更糟糕的结果。相比之下，随着任务复杂性的增加，我们的攻击表现得更好。

我们发现JSMA无法在ImageNet上找到目标L0对抗性示例，而我们的能够100%成功地找到

重要的是要认识到，模型之间的结果是不可直接比较的。例如，与MNIST分类相比，尽管L0对手必须改变10倍于MNIST分类的像素数，但ImageNet的像素数是MNIST分类的114倍，因此必须改变的像素数要小得多。

---

![image-20220402150228553](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402150228553.png)

表5 针对我们的MNIST和CIFAR模型，将目标攻击的三种变体与之前的工作进行比较。当成功率不是100%时，平均值在成功的样本上统计

---

*生成合成数字*。对于我们的目标对手，我们可以从我们想要的任何图像开始，找到每个给定目标的对抗性示例。利用这一点，在图6中，我们展示了对一幅全黑图像的最小扰动，这幅图像需要对每一个距离度量进行分类

这个实验之前是针对L0任务[38]进行的，但是当他们发起攻击时，“对于0、2、3和5类，人们可以清楚地识别目标数字。”随着我们更强大的攻击，没有一个数字是可识别的。图7从全白色图像开始执行相同的分析

---

![image-20220402150356993](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402150356993.png)

图6. 针对10位MNIST数字中的每一位进行有针对性的攻击，其中三个距离度量的起始图像均为全黑。

![image-20220402150428831](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402150428831-16488830693753.png)

图7。针对10位MNIST数字中的每一位进行有针对性的攻击，其中起始图像在三个距离度量中均为全白色

---

请注意，全黑图像不需要更改就可以变成数字1，因为它最初被分类为1，而全白图像不需要更改就可以变成8，因为初始图像已经是8。

*运行时分析*。我们认为，有两个原因，人们可以考虑对抗示例生成算法的运行时性能：首先，了解性能是否会对对手进行实际攻击造成阻碍，以及第二，在对抗性再训练中用作内环[11]。

比较攻击的确切运行时间可能会产生误导。例如，我们对L2敌手的实现进行了并行化，允许它在GPU上同时运行数百次攻击，从而将性能从10倍提高到100倍。然而，我们没有将L0或L并行化∞ 攻击。类似地，我们的快速梯度符号实现是并行的，但JSMA不是。因此，我们避免给出确切的业绩数字，因为我们认为不公平的比较比没有比较更糟糕。

我们的所有攻击，以及之前的所有攻击，都非常有效，可以供对手使用。在任何给定实例上运行攻击所需的时间都不会超过几分钟

与L0相比，我们的攻击比我们优化的JSMA算法慢2-10倍（比未优化的版本快得多）。我们的攻击通常是比以往的$L_2$和$L_\infty$慢了10-100倍， 除了迭代梯度符号——我们的速度比之慢了10倍。

## 8. 评估防御蒸馏

蒸馏最初被认为是一种将大模型（老师）简化为较小蒸馏模型的方法[19]。在较高的层次上，蒸馏首先以标准的方式在培训集中培训教师模型。然后，我们使用教师用软标签（教师网络的输出向量）标记训练集中的每个实例。例如，手写数字7的图像的硬标签会说它被分类为7，而软标签可能会说它有80%的几率为7，20%的几率为1。然后，我们在老师的软标签上训练提炼出的模型，而不是在训练集中的硬标签上训练。蒸馏可能会提高测试集的准确性，以及较小模型学习预测硬标签的速度[19]，[30]。

防御性蒸馏使用蒸馏来提高神经网络的鲁棒性，但有两个显著的变化。首先，教师模型和蒸馏模型在大小上是相同的——防御蒸馏不会导致更小的模型。**第二，也是更重要的一点，防御性蒸馏使用大蒸馏温度（如下所述）迫使蒸馏模型对其预测更加自信**

回想一下，softmax函数是神经网络的最后一层。防御性蒸馏修改softmax函数，使其也包含温度常数T

![image-20220402151345608](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402151345608.png)

很容易看出，softmax（x，T）=softmax（x/T，1）。直观地说，升高温度会导致“较软”的最大值，降低温度会导致“较硬”的最大值。当温度极限为0时，softmax接近最大值；当极限趋于无穷大时，softmax（x）接近均匀分布

防御蒸馏分为四个步骤：

1. 在培训阶段，通过将softmax的温度设置为T来培训网络，即教师网络
2. 通过将教师网络应用于训练集中的每个实例来计算软标签，再次评估温度T下的softmax
3. 在T温度下使用softmax在软标签上训练蒸馏网络（与教师网络形状相同的网络）
4. 最后，在测试时运行蒸馏网络（对新输入进行分类）时，使用温度1

### A 现有攻击的脆弱性

我们简要调查了现有攻击在蒸馏网络上失败的原因，发现现有攻击非常脆弱，即使存在，也很容易找不到对抗性示例。

L-BFGS和Deepfool失败是因为F（·）的梯度几乎总是为零，这禁止使用标准目标函数

当我们在温度T下训练一个蒸馏网络，然后在温度1下测试它时，我们有效地使softmax的输入变大T倍。通过在训练期间最小化交叉熵，softmax的输出对于正确的类别被迫接近1.0，对于所有其他类别被迫接近0.0。因为Z（·）除以T，所以经过提取的网络将学习使Z（·）值T比其他情况下的值大。（正值被迫变大约T倍；负值乘以约T的系数，从而变得更为负值。）通过实验，我们验证了这一事实：在未蒸馏过的网络上，Z（·）（logits）的L1范数的平均值为5.8，标准偏差为6.4；在蒸馏网络（T=100）上，平均值为482，标准偏差为457。

因为Z（·）的值是100倍大，当我们在温度1下测试时，F的输出在所有组件中都变成$\epsilon$，除了置信度为1的输出类为$1-9\epsilon$，$\epsilon$很小（考虑10类）。实际上，在大多数情况下，$\epsilon$非常小，以至于32位浮点值被舍入为0。出于类似的原因，梯度非常小，当表示为32位浮点值时，它会变成0。

这导致L-BFGS最小化程序无法取得进展并终止。相反，如果我们使用之前确定的稳定目标函数运行L-BFGS，而不是Szegedy等人[46]建议的目标函数lossF，L（·），L-BFGS不会失败。另一种修复攻击的方法是设置

![image-20220402153222689](Towards Evaluating the Robustness of Neural Networks.assets\image-20220402153222689.png)

式中，T是所选的蒸馏温度。然后，最小化损失$\text{loss}_{F',l}(\cdot)$将不会失败，因为现在梯度不会因为浮点算术舍入而消失。这清楚地表明了使用损失函数作为最小化目标的脆弱性。

JSMA-F（我们的意思是攻击使用最后一层F（·）的输出）失败的原因与L-BFGS失败的原因相同：Z（·）层的输出非常大，因此softmax本质上变成了硬最大值。这是Papernot等人在论文[39]中用来攻击防御蒸馏的攻击版本。

JSMA-Z（使用logits的攻击）失败的原因完全不同。回想一下，在Z（·）版本的攻击中，我们使用softmax**的输入**来计算梯度，而不是网络的最终输出。这消除了梯度消失的任何潜在问题，但也带来了新的问题。Papernot等人[38]介绍了这种版本的攻击，但它不用于攻击蒸馏；我们在这里分析了它失败的原因。由于此攻击使用Z值，因此了解相对影响的差异非常重要。如果softmax层的最小输入为−100，然后，在softmax层之后，相应的输出实际上变为零。如果此输入从−100到−90年后，产量仍将几乎为零。但是，如果softmax层的最大输入为10，并更改为0，这将对softmax输出产生巨大影响

