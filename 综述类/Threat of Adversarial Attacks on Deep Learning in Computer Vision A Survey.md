# 对抗性攻击对计算机视觉深度学习的威胁：一项调查

## 摘要

深度学习是当前人工智能兴起的核心。在计算机视觉领域，它已成为从自动驾驶汽车到监控和安全等应用的主力军。虽然深度神经网络在解决复杂问题方面取得了惊人的成功（通常超出了人类的能力），但最近的研究表明，它们很容易受到以对输入的细微扰动为形式的对抗性攻击，从而导致模型预测不正确的输出。对于图像来说，这样的扰动通常太小而无法感知，但它们完全愚弄了深度学习模型。对抗性攻击对深度学习在实践中的成功构成了严重威胁。这一事实最近导致大量贡献流向这一方向。本文介绍了计算机视觉中针对深度学习的对抗性攻击的首次全面调查。我们回顾了设计对抗性攻击的工作，分析了此类攻击的存在，并提出了防御措施。为了强调对抗性攻击在实际情况下是可能的，我们分别回顾了在真实场景中评估对抗性攻击的贡献。最后，在回顾文献的基础上，我们对这一研究方向进行了更广泛的展望。

## 1. 简介

深度学习[1]在解决过去曾多次尝试过机器学习和人工智能社区的问题方面取得了重大突破。因此，它目前正被用于破译前所未有规模的科学难题，例如在脑回路重建[2]；DNA突变分析[3]；预测潜在药物分子的结构-活性[4]，并分析粒子加速器数据[5] [6]。深度神经网络也成为解决语音识别[7]和自然语言理解中许多挑战性任务的首选。

在计算机视觉领域，深度学习成为了人们关注的焦点。2012年，Krizhevsky等人[9]在一项极具挑战性的大规模视觉识别任务[11]上展示了基于卷积神经网络（CNN）[10]的模型令人印象深刻的性能。这项开创性的工作也为当前深度学习的普及提供了重要的支持。自2012年以来，计算机视觉界对深度学习研究做出了许多有价值的贡献，使其能够为移动应用[181]在医学[21]中遇到的问题提供解决方案。人工智能领域最近取得的突破，如AlphaGo Zero[14]的网格学习，也得益于最初为图像识别任务而提出的深度残差网络[147]。

随着深层神经网络模型的不断改进[145]、[147]、[168]；开放获取高效的深度学习软件库[177]、[178]、[179]；由于训练复杂模型所需的硬件容易获得，深度学习正在快速实现进入安全和安保关键应用的成熟，例如自动驾驶汽车[12]、[182]、监控[13]、恶意软件检测[34]、[107]、无人机和机器人[157]、[180]以及语音命令识别[7]。随着人脸识别ATM[183]和手机上的人脸ID安全[184]等现实世界的发展，很明显，深度学习解决方案，尤其是源自计算机视觉问题的解决方案，将在我们的日常生活中发挥重要作用。

虽然深度学习以惊人的精确度执行各种各样的计算机视觉任务，但Szegedy等人[22]首先在图像分类的背景下发现了深度神经网络的一个有趣弱点。他们发现，尽管现代深度网络具有很高的精确度，但它还是令人惊讶地容易受到敌对攻击，比如对人类视觉系统仍然（几乎）无法察觉的图像的小扰动。这种攻击会导致神经网络分类器完全改变其对图像的预测。更糟糕的是，受攻击的模型报告对错误预测的可信度很高。此外，同样的图像扰动可以欺骗多个网络分类器。这些研究结果的深远影响引发了研究人员对对抗性攻击及其深层学习防御的广泛兴趣。

自从Szegedy等人[22]的发现以来，关于计算机视觉中深度学习的对抗性攻击，出现了一些有趣的结果。例如，除了特定于图像的对抗性干扰[22]，Moosavi Dezhouni等人[16]证明了“普遍干扰”的存在，它可以欺骗任何图像上的网络分类器（例如，见图1）。类似地，Athalye等人[65]证明，即使是3D打印现实世界中的对象，也有可能愚弄深层神经网络分类器（见第4.3节）。鉴于计算机视觉中深度学习研究的重要性及其在现实生活中的潜在应用，本文首次全面综述了计算机视觉中对深度学习的对抗性攻击。这篇文章的读者群比计算机视觉社区更广，因此它只假设了深度学习和图像处理的基本知识。然而，它也为感兴趣的读者讨论了重要贡献的技术细节。

在第2节中，我们首先用计算机视觉的术语描述与对手攻击相关的常用术语。在第3节中，我们回顾了针对图像分类和其他任务的对抗性攻击。另一部分介绍了在现实环境中处理对抗性攻击的方法。第4节对这些方法进行了回顾。在文献中，也有一些作品主要侧重于分析对抗性攻击的存在。我们将在第5节中讨论这些贡献。第6节讨论了以防御对抗性攻击为中心的方法。在第7节中，我们根据回顾的文献对研究方向进行了更广泛的展望。最后，我们在第8节中得出结论。

## 2. 术语的定义

在本节中，我们将介绍与计算机视觉中深度学习的对抗性攻击相关的文献中使用的常用技术术语。其余的文章也遵循这些术语的相同定义。

* *对抗性示例/图像*是故意干扰的干净图像的修改版本（例如添加噪声）以迷惑/愚弄机器学习技术，如深层神经网络。
* *对抗性干扰*是添加到干净图像中的噪声，使其成为对抗性示例。
* *对抗性训练*除了使用干净的图像外，还使用对抗性图像来训练机器学习模型。
* *“对手”*通常指创建对抗性示例的代理。然而，在某些情况下，这个例子本身也被称为对手
* *黑盒攻击*为目标模型提供了（在测试期间）在不了解该模型的情况下生成的通用示例。在某些情况下，假设对手对模型的了解有限（例如，其训练程序和/或体系结构），但肯定不知道模型参数。在其他情况下，使用有关目标模型的任何信息被称为“半黑箱”攻击。我们在本文中使用前一种约定。
* *检测器*是一种（仅）检测图像是否为敌对示例的机制。
* *愚弄比率/比率*表示图像受到干扰后，经过训练的模型更改其预测标签的图像百分比
* *一次/一步方法*通过执行单步计算（例如，计算一次模型损失梯度）产生对抗性扰动。相反的是*迭代方法*，它们多次执行相同的计算以获得单个扰动。后者通常在计算上很昂贵
* *准潜移默化的扰动*对人类感知的图像影响非常小
* *整流器*修改对抗性示例，将目标模型的预测**恢复**为同一示例的干净版本上的预测
* *目标攻击*会欺骗模型，使其错误地预测敌对图像的特定标签。它们与*非目标攻击*相反，在非目标攻击中，敌对图像的预测标签无关紧要，只要它不是正确的标签。
* *威胁模型*是指一种方法所考虑的潜在攻击类型，例如黑盒攻击。
* *可转移性*指的是一个对抗性示例即使对于生成它的模型以外的其他模型仍然有效的能力。
* *普遍 扰动*能够以很高的概率在“任何”图像上愚弄给定的模型。注意，普适性指的是微扰的性质是“**图像不可知的**”，而不是具有良好的可转移性。
* *白盒攻击*假设目标模型的完整知识，包括其参数值、体系结构、训练方法，以及在某些情况下的训练数据

## 3. 对抗攻击

在本节中，我们将回顾计算机视觉领域的大量文献，这些文献介绍了针对深度学习的对抗性攻击方法。综述的文献主要涉及**在“实验室”**中愚弄深层神经网络的艺术，其中为典型的计算机视觉任务（例如识别）开发了方法，并使用标准数据集（例如MNIST）证明了其有效性[10]。第4节分别回顾了**在现实环境**中攻击深度学习的技术。然而，应该注意的是，本节中回顾的方法构成了现实世界攻击的基础，几乎每一种方法都有可能在实践中显著影响深度学习。我们的师是根据原始贡献中的攻击条件进行评估的。

本节的回顾主要是按**时间逻辑顺序**组织的，很少有例外，以保持讨论的流畅性。为了向读者提供对核心概念的技术理解，我们还深入研究了流行方法的技术细节，以及该领域新兴方向的一些代表性技术。还简要讨论了其他方法。关于这些技术的细节，我们参考了原始文件。本节分为两部分。在第3.1部分中，我们回顾了攻击执行计算机视觉中最常见任务（即分类/识别）的深层神经网络的方法。第3.2部分讨论了主要设计用于攻击此任务之外的深度学习的方法。

### 3.1 对分类的攻击

#### 3.1.1 盒约束下的L-BFGS

Szegedy等人[22]首先证明了图像存在小扰动，因此扰动的图像可能会愚弄深度学习模型，使其误分类。让$I_c\in \R^m$代表一个向量化的干净图片，下标$c$表示“干净”。计算加性扰动$\rho\in \R^m$会轻微扭曲图像以欺骗网络，Szegedy等人提出解决以下问题：

![image-20220331111638717](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331111638717.png)

---

![image-20220331111742288](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331111742288.png)

图2:AlexNet[9]使用[22]生成的对抗性示例的说明。为了更好地显示，扰动被放大10倍（值偏移128并被钳制）。还显示了对抗性示例的预测标签

---

其中$l'$代表图片标签，$\mathcal{C}$代表深度网络分类器。作者建议求解（1）的非平凡解，其中$l$不同于$I_c$的原始标签。在这种情况下，（1）成为一个困难的问题，因此使用盒约束的L-BFGS[20]寻求近似解。这是通过找到**最小的**（疑问：怎么感觉应该是最大呢？）c>0来实现的——使得以下式子的最小所对应的扰动$\rho$能满足条件$\mathcal{C}(I_c+\rho)=l$：

![image-20220331112027459](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331112027459.png)

其中L（，）计算分类器的损失。我们注意到（2）对于具有凸损失函数的分类器能得到精确解。然而，对于深层神经网络来说，通常情况并非如此。计算出的扰动被简单地添加到图像中，使其成为一个对抗性的例子。

（注：c越大，越在意模大小而不在意Loss，对应的$\rho$会越小，就越不可能使得误分类条件得到满足）

如图2所示，上述方法能够计算干扰，当添加到干净的图像时，干扰会愚弄神经网络，但敌对图像与人类视觉系统的干净图像相似。Szegedy等人观察到，为一个神经网络计算的扰动也能够愚弄多个网络。这些惊人的结果发现了深度学习的一个盲点。在这一发现时，计算机视觉界正在迅速适应这样一种印象，即深度学习功能定义了感知距离与欧几里德距离非常接近的空间。因此，这些相互矛盾的结果引发了研究人员对计算机视觉深层学习的对抗性攻击的广泛兴趣。

#### 3.1.2 快速梯度符号法（FGSM）

Szegedy等人[22]观察到，对抗性训练可以提高深层神经网络对对抗性示例的鲁棒性。为了实现有效的对抗训练，古德费罗等人[23]开发了一种方法，通过解决以下问题，有效地计算给定图像的对抗干扰：

![image-20220331131056438](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331131056438.png)

其中$\Delta\mathcal{J}(.,.,.)$计算模型参数$\theta\ \text{w.r.t.}\ I_c $当前值周围的成本函数梯度

（注意这里是最大化损失，是无目标的）

符号（.）表示符号函数，$\epsilon$是限制扰动范数的小标量值。求解（3）的方法在原著中称为“快速梯度符号法”（FGSM）。

有趣的是，FGSM生成的对抗性示例利用了高维空间中深层网络模型的“线性”，而这种模型在当时通常被认为是高度非线性的。古德费罗等人[23]假设，现代深层神经网络的设计（有意）鼓励计算增益的线性行为，也使它们容易受到廉价分析扰动的影响。在相关文献中，这一观点通常被称为“线性假设”，这一假设由FGSM方法证实。

Kurakin等人[80]指出，在流行的大规模图像识别数据集ImageNet[11]上，FGSM生成的对抗性示例的top-1错误率约为63−69% 对于$\epsilon$∈ [2, 32]。

作者还提出了FGSM的“一步目标类”变体，其中不使用（3）中图像的真实标签，他们使用的标签是$l_{target}$网络预测$I_c$的最不可能的目标类别。然后从原始图像中减去计算出的扰动，使其成为对抗性示例。对于具有交叉熵损失的神经网络，这样做可以最大化网络预测$l_{target}$作为对抗性示例标签的概率。有人建议，一个随机类也可以用作愚弄网络的目标类，但它可能会导致不那么有趣的愚弄，例如将一个品种的狗误分类为另一个品种的狗。作者还证明，对抗性训练提高了深层神经网络对FGSM及其拟议变体产生的攻击的鲁棒性

FGSM会扰动图像，从而增加结果图像上分类器的损失。符号函数确保损失的大小最大化，而$\epsilon$本质上限制了无穷范数。Miyato等人[103]提出了一种密切相关的方法来计算扰动，如下所示

![image-20220331132633390](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331132633390.png)

在上面的等式中，计算的梯度是正态的，用它的“2-范数”表示。Kurakin等人[80]将该技术称为“快速梯度L2”方法，并提出了使用`∞-标准化的选项，并将结果技术称为“快速梯度L∞” 方法。广义而言，所有这些方法在与计算机视觉中的对抗性攻击相关的文献中都被视为**“一步”或“一次性”方法**

#### 3.1.3 基本&最小可能类迭代法

一步方法通过在增加分类器损失的方向上迈出一大步（即一步梯度上升）来扰动图像。这个想法的一个直观扩展是迭代地采取多个小步骤，在每一步之后调整方向。基本迭代法（BIM）[35]正是这样做的，并迭代计算以下内容：

![image-20220331133012592](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331133012592.png)

其中$I_\rho^i$代表第$i$次扰动之后的图片，$\text{Clip}_\epsilon\{\cdot\}$代表截断图像像素值，$\alpha$代表步长（一般为1）。BIM算法从$I^0_\rho=I_c$开始，并按照公式$\lfloor \min(\epsilon+4,1.25\epsilon)\rfloor$确定的迭代次数运行。Madry等人[55]指出，BIM等同于无穷范数版本的投影梯度下降（PGD），是一种标准的凸优化方法。

与将FGSM扩展到其“一步目标类”变体类似，Kurakin等人[35]也将BIM扩展到迭代最小可能类方法（ILCM）。在这种情况下，将（5）中图像的标签替换为分类器预测的最不可能类别的目标标签。ILCM方法产生的对抗性示例已被证明严重影响了现代深层体系结构Inception v3[145]的分类精度，即使是非常小的$\epsilon$值，例如<16。

#### 3.1.4 基于雅可比矩阵的显著图攻击（JSMA）

在文献中，更常见的是通过限制无穷或2-范数，使人类无法察觉。然而，Papernot等人[60]也通过限制扰动的“0-范数”创建了对抗性攻击。从物理上讲，这意味着目标是只修改图像中的几个像素，而不是干扰整个图像来愚弄分类器。他们生成所需对抗图像的算法的关键可以理解如下。该算法一次修改一个干净图像的像素，并监控变化对结果分类的影响。通过使用网络层输出的梯度计算显著性图，来执行监控。在该图中，较大的值表示欺骗网络预测“目标作为修改图像的标签而不是原始标签”的可能性更高。因此，该算法执行有目标的愚弄。一旦计算出地图，算法就会选择最有效的像素来愚弄网络并改变它。重复这个过程，直到敌对图像中允许的最大像素数被改变，或者愚弄成功。

#### 3.1.5 单像素攻击

对抗性攻击的一个极端情况是，仅更改图像中的一个像素以欺骗分类器。有趣的是，Su等人[68]声称，在70.97%的测试图像上，通过改变每幅图像的一个像素，成功地愚弄了三种不同的网络模型。他们还报告称，网络对错误标签的平均置信度为97.47%。我们在图3中展示了[68]中敌对图像的代表性示例。Su等人使用差异进化的概念计算了对抗性例子[148]。对于一个干净的图片$I_c$，他们首先创造了一个$\R^5$上400向量的几何，使得每一个向量包含$x,y$和RGB值，对应一个像素。然后，他们随机修改向量的元素以创建子对象，这样子对象在下一次迭代中与其父对象竞争适应度，而网络的概率预测标签被用作适应度标准。最后一个幸存的孩子被用来改变图像中的像素

---

![image-20220331134519838](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331134519838.png)

图3：单像素对抗性攻击的图示[68]：每张图像都提到了正确的标签。括号中给出了相应的预测标签。

---

即使采用如此简单的进化策略，Su等人[68]也成功地愚弄了深层网络。请注意，差分进化使他们的方法能够生成对抗性示例，**而无需获取有关网络参数值或其梯度的任何信息**。他们的技术需要的唯一输入是目标模型预测的**概率标签**。

#### 3.1.6 卡里尼和瓦格纳攻击（C&W）

卡里尼和瓦格纳[36]在对敌对干扰进行防御升级后，发起了一系列三次敌对攻击[38]。这些攻击通过限制它们的三种范数，结果表明，针对目标网络的防御蒸馏几乎完全无法抵御这些攻击。此外，它还表明，使用不安全（未蒸馏）网络生成的对抗示例可以很好地转移到安全（蒸馏）网络，这使得计算出的扰动适用于黑盒攻击。

虽然更常见的是利用对抗性示例的转移能力特性来生成黑盒攻击，但Chen等人[41]也提出了基于“零阶优化（ZOO）”的攻击，直接估计目标模型的梯度以生成对抗性示例。这些攻击的灵感来自C&W攻击。关于C&W和动物园袭击的更多细节，我们参考原始文件。

#### 3.1.7 深度愚弄

Moosavi Dezhouni等人[72]提出以迭代方式计算给定图像的最小范数对抗性扰动。他们的算法，即DeepFool使用干净的图像进行初始化，该图像被假定位于受分类器决策边界限制的区域内。该区域决定图像的类别标签。在每次迭代中，该算法通过计算一个小向量来扰动图像，从而将生成的图像带到多边形的边界，该边界是通过将图像所在区域的边界线性化而获得的。一旦扰动图像根据网络的原始决策边界改变其标签，在每次迭代中添加到图像的扰动被累积以计算最终扰动。作者表明，DeepFool算法能够计算出比FGSM[23]计算的扰动范数更小的扰动，同时具有相似的愚弄率。

#### 3.1.8 普遍对抗性扰动

FGSM[23]、ILCM[35]、Deep Fool[72]等方法通过计算扰动来愚弄单个图像上的网络，而Moosavi Dezhouni等人[16]计算的“通用”对抗扰动能够以高概率愚弄“任何”图像上的网络。如图1所示，这些图像不可知的扰动对于人类视觉系统来说仍然是不可察觉的。为了正式定义这些扰动，让我们假设从分布$\mathfrak{\hat s}_c$中采样干净的图像。如果扰动ρ满足以下约束，则它是“通用的”：

![image-20220331135645062](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331135645062.png)

其中P（.）表示概率δ∈ （0，1）是愚弄比率，$||\cdot||_p$代表了$l_p$范数，$\xi$是限制。ξ的值越小，就越难察觉图像中的扰动。严格地说，满足（6）的扰动应被称为（δ，ξ）-普适，因为它们强烈依赖于上述参数。然而，这些干扰在文献中通常被称为“普遍对抗性干扰”

作者通过限制他们的“2-范数”以及`∞-范数计算扰动，并表明，对于最先进的图像分类器，范数上限为各自图像范数的4%的扰动已经达到了约0.8或更高的显著愚弄率。他们计算扰动的迭代方法与DeepFool策略[72]有关，该策略将数据点（即图像）逐渐推送到其类别的决策边界。然而，在这种情况下，“所有”训练数据点被依次推送到各自的决策边界，并且通过每次将累加器反向投影到所需的半径为ξ的“p球”，在所有图像上计算的扰动逐渐累积。

Moosavi Dezhouni等人[16]提出的算法在针对单一网络模型时计算扰动，例如ResNet[147]。然而，研究表明，这些扰动也可以很好地推广到不同的网络（尤其是那些具有类似体系结构的网络）。从这个意义上说，作者声称扰动在某种程度上是“双重普遍的”。此外，还表明高愚弄率（例如δ≥ 0.5）可以通过仅使用大约2000张训练图像来学习扰动来实现。

Khrulkov等人[190]还提出了一种方法，将普遍对抗性扰动构造为网络特征映射的雅可比矩阵的奇异向量，这使得仅使用少量图像就能获得相对较高的愚弄率。另一种产生普遍扰动的方法是Mopuri等人[135]提出的快速特征愚弄法。他们的方法产生了与数据无关的普遍扰动

#### 3.1.9 沮丧UPSET和焦虑ANGRI

Sarkar等人[146]提出了两种**黑盒攻击算法**，即UPSET：用于转向精确目标的通用扰动，以及ANGRI：用于生成恶意图像以有针对性地愚弄深层神经网络的对抗网络。对于“n”类，UNDP寻求产生“n”图像不可知扰动，这样当扰动添加到不属于目标类的图像时，分类器将把扰动图像分类为来自该类。UPSET的力量来自剩余的生成网络R（.），它将目标类“t”作为输入，并生成一个用于愚弄的扰动R（t）。整体方法使用所谓的扰动网络解决以下优化问题：

![image-20220331140622545](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331140622545.png)

其中Ic中的像素值被标准化为[−1，1]，s是一个标量。为了确保Iρ是一个有效图像，所有值都在间隔之外[−1，1]被剪掉了。与颠覆的图像不可知扰动相比，ANGRI以一种密切相关的方式计算特定于图像的扰动，我们参考了原始工作。ANGRI产生的扰动也被用于有针对性的愚弄。据报道，这两种算法在MNIST[10]和CIFAR-10[152]数据集上都实现了高愚弄率。

#### 3.1.10 胡迪尼

西塞等人[131]提出了“Houdini”，这是一种通过生成可根据任务损失定制的建议示例来愚弄基于梯度的学习机的方法。生成对抗性示例的典型算法使用网络可微损失函数的梯度来计算扰动。然而，这种方法往往无法弥补任务损失。例如，语音识别的任务损失基于单词错误率，这不允许直接利用损失函数梯度。Houdini是专门为此类任务生成对抗性示例的。除了成功生成用于分类的敌对图像外，Houdini还成功地攻击了一个流行的深度自动语音识别系统[151]。作者还通过在黑盒攻击场景中愚弄谷歌语音，演示了语音识别中攻击的可转移性。此外，针对人体姿势估计的深度学习模型，还展示了成功的目标攻击和非目标攻击。

#### 3.1.11 对抗性转换网络（ATN）

Baluja和Fischer[42]训练的前馈神经网络可以生成对抗其他目标网络或一组网络的对抗性示例。经过训练的模型被称为对抗性转换网络（ATN）。通过最小化由两部分组成的联合损失函数来计算这些网络生成的对抗性示例。第一部分限制对抗性示例与原始图像具有感知相似性，而第二部分旨在改变目标网络对结果图像的预测

沿着同一方向，Hayex和Danezis[47]还使用攻击者神经网络学习用于黑匣子攻击的对抗性样本。在给出的结果中，攻击者网络计算的示例与干净的图像在本质上是无法区分的，但它们被目标网络以压倒性的概率错误分类——在MNIST数据[10]上，分类准确率从99.4%降低到0.77%，在CIFAR-10数据集[152]上，分类准确率从91.4%降低到6.8%

#### 3.1.12 杂项攻击

上面讨论的对抗性攻击要么是最近文献中流行的攻击，要么是代表了迅速流行的研究方向。表1还提供了这些攻击的主要属性摘要。为了进行全面的研究，下面我们简要介绍了在深层神经网络上产生对抗性攻击的进一步技术。我们注意到这个研究领域目前非常活跃。虽然我们已尽一切努力审查尽可能多的方法，但我们并不认为审查是详尽无遗的。**由于这一研究方向的活跃程度很高，在不久的将来可能会出现更多的攻击**。

Sabour等人[26]展示了通过改变深层神经网络的内部层来产生对抗性例子的可能性。作者证明，可以使敌对图像的内部网络表示，类似于不同类别图像的表示。Papernot等人[109]研究了用于深度学习的对抗性攻击以及其他机器学习技术的可转移性，并引入了进一步的可转移性攻击。Narodytska和Kasiviswanathan[54]还引入了更多的黑盒攻击，这些攻击通过只改变图像中的几个像素值，有效地愚弄了神经网络。Liu等人[31]引入了“epsilon，Neighborial”攻击，该攻击已被证明可以愚弄防御网络[108]，对白盒攻击的成功率为100%。Oh等人[133]从“博弈论”的角度研究了对抗性攻击，并得出了一种策略来对抗针对深层神经网络的对抗性攻击所采取的对抗措施。Mpouri等人[135]开发了一种独立于数据的方法，为深度网络模型生成通用的对抗性扰动。Hosseini等人[98]引入了“语义对抗性示例”的概念，即输入图像代表人类语义相同的对象，但深层神经网络将其错误分类。他们使用图像的负片作为语义对抗的例子。Kanbak等人[73]在DeepFool方法[72]之后引入了“Manibool”算法，以测量深度神经网络对几何扰动图像的鲁棒性。Dong等人[170]提出了一种迭代方法来增强针对黑匣子场景的对抗性攻击。最近，Carlini和Wagner[59]还证明，使用新的损失函数构造的新攻击可以再次击败针对扰动的十种不同防御。Rozsa等人[94]还提出了一种“热/冷”方法，用于为单个图像生成多个可能的对抗性示例。有趣的是，对抗性干扰不仅被添加到图像中以降低深度学习分类器的准确性。Yoo等人[195]最近提出了一种方法，通过对图像的细微扰动，也可以**略微提高**分类性能

---

![image-20220331143141457](Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.assets\image-20220331143141457.png)

表1：各种攻击方法的属性摘要：“扰动范数”表示扰动的受限“p范数”，以使其不易察觉。强度（星号越高，强度越高）是基于综述文献的印象。

---

我们注意到，本文中回顾的许多作品的作者已经公开了其实现的源代码。这是目前这一研究方向兴起的主要原因之一。除了这些资源之外，还有一些图书馆，例如Cleverhans[111]，[112]已经开始出现，以进一步推动这一研究方向。对抗性运动场(https://github.com/QData/addarialdnn游乐场）是Norton和Qi[142]为理解对抗性攻击而公开的工具箱的另一个例子

### 3.2 分类/识别之外的攻击

除了胡迪尼[131]之外，第3.1节中回顾的所有主流对抗攻击都直接集中在分类任务上——通常是愚弄基于CNN的[10]分类者。然而，由于对抗性威胁的严重性，除了计算机视觉中的分类/识别任务外，也正在积极调查攻击。在这里，我们回顾了一些研究如何攻击深层神经网络的工作。

#### 3.2.1 对自动编码器和生成模型的攻击

Tabacof等人[128]研究了针对自动编码器的对抗性攻击[154]，并提出了一种扭曲输入图像（使其具有对抗性）的技术，从而误导自动编码器重建完全不同的图像。他们的方法攻击神经网络的内部表示，从而使敌对图像的表示与目标图像的表示相似。然而，据[128]报道，与典型的分类器网络相比，自动编码器似乎对对抗性攻击更具鲁棒性。Kos等人[121]还探索了计算深层生成模型对抗性示例的方法，例如变分自动编码器（VAE）和VAE生成对抗性网络（VAE-GANs）。例如[153]现在在计算机视觉应用中非常流行，因为它们能够学习数据分布并使用这些分布生成逼真的图像。作者介绍了针对VAE和VAE GANs的三种不同类型的攻击。由于这些攻击的成功，我们可以得出结论，**深层生成模型也能说服对手将输入转化为非常不同的输出**。这项工作进一步支持了“对抗性示例是当前神经网络架构的普遍现象”的假设。